{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unzipping the files \n",
    "for f in os.listdir('legaldata'):\n",
    "    #print(f)\n",
    "    new_dir = os.path.join('legaldata',f)\n",
    "    #print(new_dir)\n",
    "    for file in os.listdir(new_dir):\n",
    "        new_dir2 = os.path.join(new_dir,file)\n",
    "        #print(new_dir2)\n",
    "        if new_dir2.endswith('.zip'):\n",
    "            try:\n",
    "                ZipFile(new_dir2).extractall(os.path.join(os.path.splitext(new_dir2)[0]))\n",
    "                os.remove(new_dir2)\n",
    "                \n",
    "            except:\n",
    "                    print(new_dir2)\n",
    "                    os.remove(new_dir2)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinance = {'01':'Alabama','02':'Alaska','03':'Arizona','04':'Arkansas','05':'California','06':'Colorado','07':'Connecticut',\n",
    "          '08':'Delaware','09':'District of Columbia','10':'Florida','11':'Georgia','12':'Guam & Northern Mariana Islands',\n",
    "          '13':'Hawaii','14':'Idaho','15':'Illinois','16':'Indiana','17':'Iowa','18':'Kansas','19':'Kentucky','20':'Louisiana',\n",
    "          '21':'Maine','22':'Maryland','23':'Massachusetts','24':'Michigan','25':'Minnesota','26':'Mississippi','27':'Missouri',\n",
    "          '28':'Montana','29':'Nebraska','30':'Nevada','31':'New Hampshire','32':'New Jersey','33':'New Mexico','34':'New York',\n",
    "          '35':'North Carolina','36':'North Dakota','37':'Ohio','38':'Oklahoma','39':'Oregon',\n",
    "          '40':'Pennsylvania','41':'Puerto Rico','42':'Rhode Island','43':'South Carolina','44':'South Dakota','45':'Tennessee',\n",
    "           '46':'Texas','47':'Utah','48':'Vermont','49':'Virgin Islands','50':'Virginia','51':'Washington','52':'West Virginia',\n",
    "         '53':'Wisconsin','54':'Wyoming'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annoting the files \n",
    "import shutil \n",
    "for f in os.listdir('legaldata'):\n",
    "    #print(f)\n",
    "    new_dir = os.path.join('legaldata',f)\n",
    "    #print(new_dir)\n",
    "    i = 1\n",
    "    for doc in os.listdir(new_dir):\n",
    "        new_dir2 = os.path.join(new_dir,doc)\n",
    "        #print(new_dir2)\n",
    "        for k,v in provinance.items():\n",
    "            if f == v:\n",
    "                dst = k + str('-05-') + str(i) + ' '+ doc[5:]\n",
    "                #print(dst)\n",
    "                i = i+1\n",
    "                dst1 = new_dir + dst\n",
    "                #print(dst1)\n",
    "                src = new_dir + doc\n",
    "                #print(src)\n",
    "                os.rename(os.path.join(new_dir,doc),os.path.join(dst))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mammoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convering doc file to docx \n",
    "from glob import glob\n",
    "import re\n",
    "import win32com.client as win32\n",
    "from win32com.client import constants\n",
    "\n",
    "def save_as_docx(path):\n",
    "        # Opening MS Word\n",
    "        word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "        doc = word.Documents.Open(path)\n",
    "        doc.Activate ()\n",
    "        \n",
    "        # Rename path with .docx\n",
    "        new_file_abs = os.path.abspath(path)\n",
    "        new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "        \n",
    "        # Save and Close\n",
    "        word.ActiveDocument.SaveAs(\n",
    "            new_file_abs, FileFormat=constants.wdFormatXMLDocument\n",
    "        )\n",
    "        doc.Close(False)\n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of paths to .doc files\n",
    "main = os.path.join(os.getcwd(),'legaldoc')\n",
    "for file in os.listdir(main):\n",
    "    paths = os.path.join(os.path.join(main),file)\n",
    "    save_as_docx(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.getcwd(),'training')\n",
    "print(path)\n",
    "for d in os.listdir(path):\n",
    "    p = os.path.join(path,d)\n",
    "    #print(p)\n",
    "    filename = d.split(\".\")[0]\n",
    "    #print(filename)\n",
    "MY_TEXT = docx2txt.process(p)\n",
    "with open(filename + '.txt', \"w\") as text_file:\n",
    "    print(MY_TEXT, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert docx file to HTML\n",
    "import mammoth\n",
    "path = os.path.join(os.getcwd(),'legaldocx')\n",
    "for file in os.listdir(path):\n",
    "    #print(f)\n",
    "    P = os.path.join(path,file)\n",
    "    #print(P)\n",
    "    f = open(P, 'rb')\n",
    "    #print(f)\n",
    "    b = open(file.split('.')[0] + '.html', 'wb')\n",
    "    #print(b)\n",
    "    doc = mammoth.convert_to_html(f)\n",
    "    b.write(doc.value.encode('utf8'))\n",
    "    f.close()\n",
    "    b.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## parsing text data from HTML file\n",
    "\n",
    "\n",
    "dir_path = 'C:\\JP_NOTEBOOKS\\output'\n",
    "results_dir = 'C:\\JP_NOTEBOOKS\\Parsed'\n",
    "\n",
    "for file_name in glob.glob(os.path.join(dir_path, \"*.html\")):\n",
    "    with open(file_name,encoding = \"utf8\" ) as html_file:\n",
    "        soup = BeautifulSoup(html_file,'lxml')\n",
    "        #print(soup.prettify())\n",
    "        \n",
    "        \n",
    "    results_file = os.path.splitext(file_name)[0] + '.txt'\n",
    "    with open(os.path.join(results_dir, results_file), 'w', encoding = 'utf-8') as outfile: \n",
    "        for tag in soup.find_all('p'):\n",
    "            #print(tag.text)\n",
    "            outfile.write(tag.text + '\\n')\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Sentence boundry detection ###\n",
    "import pysbd\n",
    "\n",
    "path = os.path.join(os.getcwd(),'new')\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    subpath = os.path.join(path,f)\n",
    "    with open(subpath,'r',encoding = \"utf8\") as file:\n",
    "        content = file.read()\n",
    "        #print(content)\n",
    "        seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "        print(seg.segment(content),'\\n')\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting AllenNLP\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl\n",
      "Collecting conllu==1.3.1 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
      "Collecting word2number>=1.1 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Collecting flaky (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: nltk in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (3.4.4)\n",
      "Collecting jsonpickle (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/6b/fbb2d499b96861a18c1641f6fefe775110d3faba65c1524950e9ad64824a/jsonpickle-1.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: boto3 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (1.9.234)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (3.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (1.16.4)\n",
      "Collecting parsimonious>=0.8.0 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz\n",
      "Collecting spacy<2.2,>=2.1.0 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/ef/b6526a755050ddd937ef88e4a969ed9ba6854f3dd792516a692aad4bb6a7/spacy-2.1.9-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: flask>=1.0.2 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (1.1.1)\n",
      "Collecting pytorch-pretrained-bert>=0.6.0 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl\n",
      "Requirement already satisfied: h5py in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (2.9.0)\n",
      "Collecting ftfy (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz\n",
      "Requirement already satisfied: pytest in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (5.0.1)\n",
      "Requirement already satisfied: requests>=2.18 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (2.22.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (2019.1)\n",
      "Collecting tensorboardX>=1.2 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (0.9.1)\n",
      "Collecting unidecode (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl\n",
      "Collecting pytorch-transformers==1.1.0 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.19 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (4.32.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (1.2.1)\n",
      "Collecting overrides (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (0.21.2)\n",
      "Collecting responses>=0.7 (from AllenNLP)\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: gevent>=1.3.6 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from AllenNLP) (1.4.0)\n",
      "Collecting sqlparse>=0.2.4 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\n",
      "Collecting flask-cors>=3.0.7 (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting editdistance (from AllenNLP)\n",
      "  Using cached https://files.pythonhosted.org/packages/5a/21/3178b32699c94aff68239372e30e01b2707f6b5438d8732d4356162fa3b1/editdistance-0.5.3-cp37-cp37m-win_amd64.whl\n",
      "Collecting torch>=1.2.0 (from AllenNLP)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement torch>=1.2.0 (from AllenNLP) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch>=1.2.0 (from AllenNLP)\n"
     ]
    }
   ],
   "source": [
    "pip install AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
